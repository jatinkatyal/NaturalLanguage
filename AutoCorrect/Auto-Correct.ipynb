{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0736fe73-08bb-4aa4-aff0-c81194dbada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '.\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e1662d6-5ee2-4040-9229-19315cdcfe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed7e7e19-c611-48cc-a222-9c7a285e7fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary by reading articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 740/740 [00:00<00:00, 7913.68it/s]\n"
     ]
    }
   ],
   "source": [
    "class ProbablisticAutoCorrect():\n",
    "    def __init__(self, data_path=DATA_PATH):\n",
    "        with open(os.path.join(DATA_PATH,'info.json'),'r') as infoFP:\n",
    "            self.info = json.load(infoFP)\n",
    "        self.wordProbs = self.returnWordProbabilities()\n",
    "        self. tokenizer = WordPunctTokenizer()\n",
    "    \n",
    "    def returnArticle(self, articleNameKey):\n",
    "        articlePath = self.info[articleNameKey]['path']\n",
    "        with open(articlePath,'r',encoding=\"utf-8\") as articleFP:\n",
    "            article = ''.join(articleFP.readlines())\n",
    "        return article\n",
    "\n",
    "    def returnArticles(self):\n",
    "        articles = []\n",
    "        vocab = {}\n",
    "        print('Building vocabulary by reading articles...')\n",
    "        for articleNameKey in tqdm(self.info.keys()):\n",
    "            articles.append(self.returnArticle(articleNameKey))\n",
    "        articles = ''.join(articles)\n",
    "        return articles\n",
    "    \n",
    "    def returnVocabulary(self):\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit([self.returnArticles()])\n",
    "        return vectorizer.vocabulary_\n",
    "\n",
    "    def returnWordProbabilities(self):\n",
    "        vocab = self.returnVocabulary()\n",
    "        totalWordCount = sum(vocab.values())\n",
    "        wordProbs = {}\n",
    "        for word in vocab.keys():\n",
    "            wordProbs[word] = vocab[word]/totalWordCount\n",
    "        return wordProbs\n",
    "\n",
    "    def identifyErrors(self, targetText):\n",
    "        tokens = self.tokenizer.tokenize(targetText)\n",
    "        potentialErrors = [token for token in tokens if token not in self.wordProbs.keys()]\n",
    "        return potentialErrors\n",
    "\n",
    "    def nEditDistStrings(self,misspelledWord,n=2):\n",
    "        return [(vocabWord,self.wordProbs[vocabWord]) for vocabWord in self.wordProbs.keys() if edit_distance(misspelledWord,vocabWord) <= n]\n",
    "\n",
    "    def infer(self,targetString):\n",
    "        resultString = targetString[:]\n",
    "        incorrectWords = self.identifyErrors(targetString)\n",
    "        print(incorrectWords)\n",
    "        for word in incorrectWords:\n",
    "            candidates = self.nEditDistStrings(word)\n",
    "            candidates.sort()\n",
    "            print(candidates[-1][0])\n",
    "            resultString = resultString.replace(word, candidates[-1][0], 1) #last candidate is of highest probability\n",
    "        return resultString\n",
    "\n",
    "robotique = ProbablisticAutoCorrect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "557a2612-e215-4a55-bb80-d2f40ee92f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AU', ',', 'a', 'acquisd', 'prores', '.']\n",
      "할래\n",
      "할래\n",
      "할래\n",
      "acquises\n",
      "spores\n",
      "할래\n",
      "Did you meant: 할래 할래u fil du temps할래 il a acquises une personnalité et une conscience spores할래\n"
     ]
    }
   ],
   "source": [
    "testTarget = 'AU au fil du temps, il a acquisd une personnalité et une conscience prores.'\n",
    "print(\"Did you meant: {}\".format(robotique.infer(testTarget)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995a1e8-6f59-42a0-85eb-eaad3d613349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
