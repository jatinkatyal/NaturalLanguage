{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d90b698-7cef-4103-a5af-cafbb4992794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aim: Scrape data from Wikipedia to create a text corpus\n",
    "# Process:\n",
    "# Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f992580c-8e92-43d7-ae28-9818ef7a49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4d15fd-4bec-47e3-957e-d5ac8e5cc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiCategoryDataScrapper:\n",
    "    def __init__(self, url, articlePrefix, dataPath=None):\n",
    "        self.categoryURL = url\n",
    "        self.articlePrefix = prefix\n",
    "        self.categoryPageSoup = self.soup(self.request(self.categoryURL))\n",
    "        self.meta = {}\n",
    "        \n",
    "        if dataPath is None:\n",
    "            self.dataPath  = 'Data'\n",
    "            if not os.path.exists(self.dataPath):\n",
    "                os.mkdir(self.dataPath)\n",
    "        else:\n",
    "            self.dataPath  = dataPath\n",
    "\n",
    "        self.index = self.letterIndices()\n",
    "        \n",
    "    def request(self,url):\n",
    "        response = requests.get(url)\n",
    "        return response\n",
    "\n",
    "    def soup(self, response):\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    def letterIndices(self):\n",
    "        index = {}\n",
    "        soups = self.categoryPageSoup.body.find(id='mw-pages').find(class_='CategoryIndex').find_all('li')[2:]\n",
    "        for soup in soups:\n",
    "            index[soup.text] = soup.a['href']\n",
    "        return index\n",
    "    \n",
    "    def letterLinks(self,letter):\n",
    "        letterResponse = self.request(self.index[letter])\n",
    "        letterSoup = self.soup(letterResponse).body.find(class_='mw-category-group')\n",
    "        articles =  {}\n",
    "        articleSoups = letterSoup.find_all('li')\n",
    "        for articleSoup in articleSoups:\n",
    "            articles[articleSoup.text] = self.articlePrefix + articleSoup.a['href']\n",
    "        return articles\n",
    "\n",
    "    def scrapeArticle(self,articleURL):\n",
    "        articleResponse = self.request(articleURL)\n",
    "        articleSoup = self.soup(articleResponse)\n",
    "        contentSoup = articleSoup.find(class_='mw-parser-output').extract()\n",
    "        intro = []\n",
    "        for childSoup in contentSoup.children:\n",
    "            if childSoup.name == 'p':\n",
    "                intro.append(childSoup.text)\n",
    "    \n",
    "        content = []\n",
    "        restContentSoups = [childSoup for childSoup in contentSoup.children]\n",
    "        restContentSoup = None\n",
    "        while not (isinstance(restContentSoup,bs4.element.Tag) or restContentSoups==[]):\n",
    "            restContentSoup = restContentSoups.pop()\n",
    "        for childSoup in restContentSoup.children:\n",
    "            if isinstance(childSoup,bs4.element.Tag):\n",
    "                if childSoup.find(id='Voir_aussi') is not None or childSoup.find(id='Notes_et_références')is not None:\n",
    "                    break\n",
    "                else:\n",
    "                    content.append(childSoup.text)    \n",
    "        return ''.join(intro + content)\n",
    "\n",
    "    def main(self):\n",
    "        urls = {}\n",
    "        print('Identifying articles to scrape.')\n",
    "        for letter in tqdm(self.index.keys()):\n",
    "            urls[letter] = self.letterLinks(letter)\n",
    "\n",
    "        print('Scraping articles')\n",
    "        for letter in tqdm(urls.keys()):\n",
    "            #print(f'Letter {letter} progress...')\n",
    "            letterPath = os.path.join(self.dataPath,letter)\n",
    "            if not os.path.exists(letterPath):\n",
    "                os.mkdir(letterPath)\n",
    "            for articleTitle in urls[letter].keys():\n",
    "                articleURL = urls[letter][articleTitle]\n",
    "                article = self.scrapeArticle(articleURL)\n",
    "                if not articleTitle.isalnum():\n",
    "                    articlePathTitle = [c for c in articleTitle if (c.isalnum() or c in ['-', ' '])]\n",
    "                    articlePathTitle = ''.join(articlePathTitle)\n",
    "                else:\n",
    "                    articlePathTitle = articleTitle\n",
    "                articlePath = os.path.join(letterPath,articlePathTitle+'.txt')\n",
    "                with open(articlePath,'w',encoding=\"utf-8\") as articleFile:\n",
    "                    articleFile.write(article)\n",
    "                self.meta[articleTitle] = {\n",
    "                    'url':urls[letter][articleTitle],\n",
    "                    'path':articlePath\n",
    "                }        \n",
    "        print(f'Scraped {len(self.meta.keys())} articles.')\n",
    "\n",
    "        metaFilePath = os.path.join(self.dataPath,'info.json')\n",
    "        with open(metaFilePath,'w') as metaFile:\n",
    "            json.dump(scraper.meta, metaFile, indent=4)\n",
    "        print(f'Meta data present in file {metaFilePath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ade136d-27f0-4fbf-868a-cc55b0758b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryURL = 'https://fr.wikipedia.org/w/index.php?title=Cat%C3%A9gorie:Portail:Robotique/Articles_li%C3%A9s&pageuntil=Essaim+de+drones#mw-pages'\n",
    "prefix = 'https://fr.wikipedia.org'\n",
    "path = \"Data\"\n",
    "scraper = WikiCategoryDataScrapper(url=categoryURL, articlePrefix=prefix, dataPath=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d13b10-5882-4a38-a807-21f4a1cbcf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying articles to scrape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 26/26 [00:17<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 26/26 [07:53<00:00, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 740 articles.\n",
      "Meta data present in file Data\\info.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scraper.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
