{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d90b698-7cef-4103-a5af-cafbb4992794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aim: Scrape data from Wikipedia to create a text corpus\n",
    "# Process:\n",
    "# Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f992580c-8e92-43d7-ae28-9818ef7a49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4d15fd-4bec-47e3-957e-d5ac8e5cc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiCategoryDataScrapper:\n",
    "    def __init__(self, url, articlePrefix, dataPath=None):\n",
    "        self.categoryURL = url\n",
    "        self.articlePrefix = prefix\n",
    "        self.categoryPageSoup = self.soup(self.request(self.categoryURL))\n",
    "        self.meta = {}\n",
    "        \n",
    "        if dataPath is None:\n",
    "            os.mkdir('Data')\n",
    "            dataPath  = 'Data'\n",
    "            \n",
    "        self.index = self.letterIndices()\n",
    "        urls = {}\n",
    "        print('Identifying articles to scrape.')\n",
    "        for letter in tqdm(self.index.keys()):\n",
    "            urls[letter] = self.letterLinks(letter)\n",
    "        \n",
    "        for letter in urls.keys():\n",
    "            print(f'Letter {letter} progress...')\n",
    "            os.mkdir(f'{dataPath}\\{letter}')\n",
    "            for articleTitle in tqdm(urls[letter].keys()):\n",
    "                articleURL = urls[letter][articleTitle]\n",
    "                article = self.scrapeArticle(articleURL)\n",
    "                if '\\\\' in articleTitle:\n",
    "                    articlePath = f'{dataPath}/{letter}/{articleTitle.pop}.txt'    \n",
    "                articlePath = f'{dataPath}/{letter}/{articleTitle}.txt'\n",
    "                with open(articlePath,'w',encoding=\"utf-8\") as articleFile:\n",
    "                    articleFile.write(article)\n",
    "                self.meta[articleTitle] = {\n",
    "                    'url':urls[letter][articleTitle],\n",
    "                    'path':articlePath\n",
    "                }\n",
    "        \n",
    "        print(f'Scraped {len(self.meta.keys())} articles.')\n",
    "        metaData = json.dump(self.meta,indent=4)\n",
    "        with opern(f'{dataPath}\\info.json',w) as metaFile:\n",
    "            metaFile.write(metaData)\n",
    "        print(f'Meta data present in file {dataPath}\\info.json')\n",
    "    \n",
    "    def request(self,url):\n",
    "        response = requests.get(url)\n",
    "        return response\n",
    "\n",
    "    def soup(self, response):\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    def letterIndices(self):\n",
    "        index = {}\n",
    "        soups = self.categoryPageSoup.body.find(id='mw-pages').find(class_='CategoryIndex').find_all('li')[2:]\n",
    "        for soup in soups:\n",
    "            index[soup.text] = soup.a['href']\n",
    "        return index\n",
    "    \n",
    "    def letterLinks(self,letter):\n",
    "        letterResponse = self.request(self.index[letter])\n",
    "        letterSoup = self.soup(letterResponse).body.find(class_='mw-category-group')\n",
    "        articles =  {}\n",
    "        articleSoups = letterSoup.find_all('li')\n",
    "        for articleSoup in articleSoups:\n",
    "            articles[articleSoup.text] = self.articlePrefix + articleSoup.a['href']\n",
    "        return articles\n",
    "\n",
    "    def scrapeArticle(self,articleURL):\n",
    "        articleResponse = self.request(articleURL)\n",
    "        articleSoup = self.soup(articleResponse)\n",
    "        contentSoup = articleSoup.find(class_='mw-parser-output')\n",
    "        content = ''.join([line.text for line in articleSoup.find(class_='mw-parser-output').find_all(['p','ul','ol'])])\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ade136d-27f0-4fbf-868a-cc55b0758b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://fr.wikipedia.org/w/index.php?title=Cat\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mC3\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA9gorie:Portail:Robotique/Articles_li\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mC3\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA9s&pageuntil=Essaim+de+drones#mw-pages\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://fr.wikipedia.org\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m scrapper \u001b[38;5;241m=\u001b[39m \u001b[43mWikiCategoryDataScrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mWikiCategoryDataScrapper.__init__\u001b[1;34m(self, url, articlePrefix, dataPath)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataPath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     dataPath  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mletterIndices()\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'Data'"
     ]
    }
   ],
   "source": [
    "url = 'https://fr.wikipedia.org/w/index.php?title=Cat%C3%A9gorie:Portail:Robotique/Articles_li%C3%A9s&pageuntil=Essaim+de+drones#mw-pages'\n",
    "prefix = 'https://fr.wikipedia.org'\n",
    "scrapper = WikiCategoryDataScrapper(url,prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
